{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755074e6-8342-4d48-accb-d820ed6987b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain faiss-cpu sentence-transformers==2.2.2 InstructorEmbedding pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ec5da-c5c9-4a2f-8d95-21ab25e49bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00bf0e2-23d8-4cb8-9757-a1183a18d5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from pypdf import PdfReader\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "import functools\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Generator, List, Tuple\n",
    "from typing import Optional, TypeVar\n",
    "import math\n",
    "from typing import Any\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "\n",
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko\")\n",
    "\n",
    "# Define an embedding method that uses the model\n",
    "def encode_texts_to_embeddings(sentences: List[str]) -> List[Optional[List[float]]]:\n",
    "    print(\"batch size \", len(sentences))\n",
    "    try:\n",
    "        embeddings = model.get_embeddings(sentences)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception as e:\n",
    "        print(\"exception\", e)\n",
    "        return [None for _ in range(len(sentences))]\n",
    "    \n",
    "\n",
    "# Generator function to yield batches of sentences\n",
    "def generate_batches(\n",
    "    sentences: List[str], batch_size: int\n",
    ") -> Generator[List[str], None, None]:\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        yield sentences[i : i + batch_size]\n",
    "\n",
    "\n",
    "def encode_text_to_embedding_batched(\n",
    "    sentences: List[str], api_calls_per_second: int = 1, batch_size: int = 100\n",
    ") -> Tuple[List[bool], np.ndarray]:\n",
    "\n",
    "    embeddings_list: List[List[float]] = []\n",
    "    print(len(sentences))\n",
    "    print(sentences[0])\n",
    "    # Prepare the batches using a generator\n",
    "    batches = generate_batches(sentences, batch_size)\n",
    "\n",
    "    seconds_per_job = 1 / api_calls_per_second\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for batch in tqdm(\n",
    "            batches, total=math.ceil(len(sentences) / batch_size), position=0\n",
    "        ):\n",
    "            futures.append(\n",
    "                executor.submit(functools.partial(encode_texts_to_embeddings), batch)\n",
    "            )\n",
    "            time.sleep(seconds_per_job)\n",
    "\n",
    "        for future in futures:\n",
    "            embeddings_list.extend(future.result())\n",
    "\n",
    "    is_successful = [\n",
    "        embedding is not None for sentence, embedding in zip(sentences, embeddings_list)\n",
    "    ]\n",
    "    print(is_successful)\n",
    "    embeddings_list_successful = np.squeeze(\n",
    "        np.stack([embedding for embedding in embeddings_list if embedding is not None])\n",
    "    )\n",
    "    return is_successful, embeddings_list_successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15322b40-97c8-4230-8498-b24f5374da89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# block to process entire directory of pdf docs\n",
    "\n",
    "embeddings_file_path = Path(\"/home/jupyter/rbi-bot/embeddings/\")\n",
    "\n",
    "# Create a rate limit of 300 requests per minute. Adjust this depending on your quota.\n",
    "API_CALLS_PER_SECOND = 1\n",
    "# According to the docs, each request can process 5 instances per request\n",
    "ITEMS_PER_REQUEST = 100\n",
    "\n",
    "\n",
    "documents_directory = '/home/jupyter/rbi-bot/rbi-docs'\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "def split_documents(splitter, documents_1):\n",
    "    # Split the documents using the provided splitter\n",
    "    split_1 = splitter.split_text(documents_1)\n",
    "    # Create documents from the split chunks\n",
    "    #split_1 = splitter.create_documents(split_1)\n",
    "    return split_1\n",
    "\n",
    "vector_db = None\n",
    "for i, filename in enumerate(os.listdir(documents_directory)):\n",
    "    documents_1 = ''\n",
    "    # Create the full file path\n",
    "    filepath = os.path.join(documents_directory, filename)\n",
    "    \n",
    "    chunk_path = embeddings_file_path.joinpath(\n",
    "        f\"{embeddings_file_path.stem}_{filename}.json\"\n",
    "    )\n",
    "    \n",
    "    # Check if the file is a file and not a directory\n",
    "    if os.path.isfile(filepath):\n",
    "        # Open the file\n",
    "        with open(filepath, 'r') as file:\n",
    "            # Read the contents of the file\n",
    "            print(f\"Processing file {i,filename}:\")\n",
    "            reader = PdfReader(filepath)\n",
    "            for page in reader.pages:\n",
    "                documents_1 += page.extract_text()\n",
    "        # Implement embeddings\n",
    "        print(f\"extracted pages\")\n",
    "        splits_1 = split_documents(splitter,documents_1)\n",
    "        print(f\"split documents\")\n",
    "        is_successful, question_chunk_embeddings = encode_text_to_embedding_batched(\n",
    "        sentences=splits_1,\n",
    "        api_calls_per_second=API_CALLS_PER_SECOND,\n",
    "        batch_size=ITEMS_PER_REQUEST,\n",
    "        )\n",
    "        print(f\"embeddings generated :\", is_successful)\n",
    "        # Save embeddings to jsonl format\n",
    "        print(f\"saving to file :\")\n",
    "        with open(chunk_path, \"a\") as f:\n",
    "            # Append to file\n",
    "            embeddings_formatted = [\n",
    "                json.dumps(\n",
    "                    {\n",
    "                        \"id\": filename + \"_\" + str(num),\n",
    "                        \"embedding\": [str(value) for value in embedding],\n",
    "                    }\n",
    "                )\n",
    "                + \"\\n\"\n",
    "                for num,embedding in enumerate(question_chunk_embeddings)\n",
    "            ]\n",
    "            f.writelines(embeddings_formatted)\n",
    "        print(f\"saved\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
