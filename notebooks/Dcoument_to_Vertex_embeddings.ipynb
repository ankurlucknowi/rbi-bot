{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755074e6-8342-4d48-accb-d820ed6987b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain faiss-cpu sentence-transformers==2.2.2 InstructorEmbedding pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ec5da-c5c9-4a2f-8d95-21ab25e49bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04268e55-2252-45eb-941e-ab449d5dc44c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from pypdf import PdfReader\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6acc1-12b0-4d05-925a-930f50b2ddb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Generator, List, Tuple\n",
    "from typing import Optional, TypeVar\n",
    "import math\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "\n",
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko\")\n",
    "\n",
    "# Define an embedding method that uses the model\n",
    "def encode_texts_to_embeddings(sentences: List[str]) -> List[Optional[List[float]]]:\n",
    "    print(\"batch size \", len(sentences))\n",
    "    try:\n",
    "        embeddings = model.get_embeddings(sentences)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception as e:\n",
    "        print(\"exception\", e)\n",
    "        return [None for _ in range(len(sentences))]\n",
    "    \n",
    "\n",
    "# Generator function to yield batches of sentences\n",
    "def generate_batches(\n",
    "    sentences: List[str], batch_size: int\n",
    ") -> Generator[List[str], None, None]:\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        yield sentences[i : i + batch_size]\n",
    "\n",
    "\n",
    "def encode_text_to_embedding_batched(\n",
    "    sentences: List[str], api_calls_per_second: int = 1, batch_size: int = 10\n",
    ") -> Tuple[List[bool], np.ndarray]:\n",
    "\n",
    "    embeddings_list: List[List[float]] = []\n",
    "    print(len(sentences))\n",
    "    print(sentences[0])\n",
    "    # Prepare the batches using a generator\n",
    "    batches = generate_batches(sentences, batch_size)\n",
    "\n",
    "    seconds_per_job = 1 / api_calls_per_second\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for batch in tqdm(\n",
    "            batches, total=math.ceil(len(sentences) / batch_size), position=0\n",
    "        ):\n",
    "            futures.append(\n",
    "                executor.submit(functools.partial(encode_texts_to_embeddings), batch)\n",
    "            )\n",
    "            time.sleep(seconds_per_job)\n",
    "\n",
    "        for future in futures:\n",
    "            embeddings_list.extend(future.result())\n",
    "\n",
    "    is_successful = [\n",
    "        embedding is not None for sentence, embedding in zip(sentences, embeddings_list)\n",
    "    ]\n",
    "    print(is_successful)\n",
    "    embeddings_list_successful = np.squeeze(\n",
    "        np.stack([embedding for embedding in embeddings_list if embedding is not None])\n",
    "    )\n",
    "    return is_successful, embeddings_list_successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d99a2f-aec8-40ee-8880-1fea5af5711e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Single File read block\n",
    "# Specify the directory you want to read\n",
    "\n",
    "documents_1 = ''\n",
    "documents_directory = '/home/jupyter/rbi-bot/rbi-docs'\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "def split_documents(splitter, documents_1):\n",
    "    # Split the documents using the provided splitter\n",
    "    split_1 = splitter.split_text(documents_1)\n",
    "    # Create documents from the split chunks\n",
    "    #split_1 = splitter.create_documents(split_1)\n",
    "    return split_1\n",
    "\n",
    "vector_db = None\n",
    "filepath = '/home/jupyter/rbi-bot/rbi-docs/01MC01042024E0D6B768164C41678A616F743BF7426B 2.pdf'\n",
    "with open(filepath, 'r') as file:\n",
    "    # Read the contents of the file\n",
    "    reader = PdfReader(filepath)\n",
    "    for page in reader.pages:\n",
    "        documents_1 += page.extract_text()\n",
    "        print(f\"generated documents\")\n",
    "    splits_1 = split_documents(splitter,documents_1)\n",
    "    print(f\"split documents\")\n",
    "    #extension_db = FAISS.from_documents(splits_1, model)\n",
    "    #extension_db.save_local(vector_db_directory) \n",
    "    #print(splits_1)\n",
    "\n",
    "#Encode a subset of questions for validation\n",
    "\n",
    "is_successful, question_embeddings = encode_text_to_embedding_batched(\n",
    "   splits_1\n",
    ")\n",
    "\n",
    "# Filter for successfully embedded sentences\n",
    "#questions = splits_1[is_successful]\n",
    "\n",
    "DIMENSIONS = len(question_embeddings[0])\n",
    "print(DIMENSIONS)\n",
    "print(question_embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7176a492-34b3-4abc-b9c3-c29f1d0717c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Embedding creation code till here ######\n",
    "#### TODO - Write embeddings to JSONL format ####\n",
    "#### TODO - Push files to vector search index ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb004c-3205-4284-9e20-ca100cd42c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "\n",
    "\n",
    "# Directory processing block\n",
    "# Specify the directory you want to read\n",
    "documents_1 = ''\n",
    "documents_directory = '/home/jupyter/rbi-bot/rbi-docs'\n",
    "vector_db_directory = '/data/vector-store/rbi-docs-v2/'\n",
    "\n",
    "#instructor_embeddings = HuggingFaceInstructEmbeddings(\n",
    "#    model_name='hkunlp/instructor-xl', model_kwargs={}\n",
    "#)\n",
    "\n",
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko\")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "def split_documents(splitter, documents_1):\n",
    "    # Split the documents using the provided splitter\n",
    "    split_1 = splitter.split_text(documents_1)\n",
    "    # Create documents from the split chunks\n",
    "    split_1 = splitter.create_documents(split_1)\n",
    "    return split_1\n",
    "\n",
    "vector_db = None\n",
    "for i, filename in enumerate(os.listdir(documents_directory)):\n",
    "    # Create the full file path\n",
    "    filepath = os.path.join(documents_directory, filename)\n",
    "    \n",
    "    # Check if the file is a file and not a directory\n",
    "    if os.path.isfile(filepath):\n",
    "        # Open the file\n",
    "        with open(filepath, 'r') as file:\n",
    "            # Read the contents of the file\n",
    "            print(f\"Processing file {i,filename}:\")\n",
    "            reader = PdfReader(filepath)\n",
    "            for page in reader.pages:\n",
    "                documents_1 += page.extract_text()\n",
    "            # Implement embeddings\n",
    "            print(f\"extracted pages\")\n",
    "            splits_1 = split_documents(splitter,documents_1)\n",
    "            print(f\"split documents\")\n",
    "            extension_db = FAISS.from_documents(splits_1, model)\n",
    "            print(f\"embeddings generated :\")\n",
    "            if vector_db is None:\n",
    "                vector_db = extension_db\n",
    "            else:\n",
    "                vector_db.merge_from(extension_db)\n",
    "        # Save db\n",
    "        print(f\"saving to vector DD :\")\n",
    "        vector_db.save_local(vector_db_directory)\n",
    "        print(f\"saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92775300-980f-46a1-81cb-010daee9193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets==7.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e6c2b-1e73-4d29-b15f-688747dda1b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token = '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'\n",
    "# Load db\n",
    "loaded_db = FAISS.load_local(\n",
    "    '/data/vector-store/rbi-docs-v2/', instructor_embeddings, allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1557816-7ee7-465d-8271-c2b988eae8e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = 'what should be constitution of a UCB board?'\n",
    "search = loaded_db.similarity_search(question)\n",
    "search\n",
    "search_with_similarity_scores = loaded_db.similarity_search_with_score(question)\n",
    "search_with_similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88522c01-0b3b-472b-993f-380c966c2c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temperature = 0.5\n",
    "max_length = 300\n",
    "llm_model = 'tiiuae/falcon-7b-instruct'\n",
    "\n",
    "# Load LLM\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=llm_model,\n",
    "    model_kwargs={'temperature': temperature, 'max_length': max_length},\n",
    "    huggingfacehub_api_token=token\n",
    ")\n",
    "\n",
    "# Create the chatbot\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=loaded_db.as_retriever(),\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5353c8-150a-4d23-a26f-c11cc94b6328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = qa({'query': question})\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
