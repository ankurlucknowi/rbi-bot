{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755074e6-8342-4d48-accb-d820ed6987b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain faiss-cpu sentence-transformers==2.2.2 InstructorEmbedding pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ec5da-c5c9-4a2f-8d95-21ab25e49bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d830470a-d2f2-48cb-88d7-d4b15806ff1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file (0, 'NT6449FC6A865BD345A2917B9386D516C3D2.pdf'):\n",
      "extracted pages\n",
      "split documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee667444dd61410c83404829cffc09f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size  25\n",
      "embeddings generated : [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "saving to file :\n",
      "saved\n",
      "Processing file (1, 'NT141FE25734050D34BE7BED8C87283466104.pdf'):\n",
      "extracted pages\n",
      "split documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3768ccd23434c49badd4571ecf2e36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size  16\n",
      "embeddings generated : [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "saving to file :\n",
      "saved\n",
      "Processing file (2, 'CIRCULARCCB6DB27B9062D14007BD700245BE816F26.pdf'):\n",
      "extracted pages\n",
      "split documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94a677851514e0d888ce1cbad43c048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size  4\n",
      "embeddings generated : [True, True, True, True]\n",
      "saving to file :\n",
      "saved\n",
      "Processing file (3, 'NT78C4A9C75F684443139BB60588DA825376.pdf'):\n",
      "extracted pages\n",
      "split documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9037207888b4da7a6c2475d4b7fec88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size  4\n",
      "embeddings generated : [True, True, True, True]\n",
      "saving to file :\n",
      "saved\n",
      "Processing file (4, 'NT5601B310BAFFA0464F9164FDE854402564.pdf'):\n",
      "extracted pages\n",
      "split documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1a9e24adfb4075b8a1c0797a89989b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size  4\n",
      "embeddings generated : [True, True, True, True]\n",
      "saving to file :\n",
      "saved\n",
      "Processing file (5, 'NT875A7981CA1BA942D1AA8ACF9DA1D7FDDD.pdf'):\n",
      "extracted pages\n",
      "split documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30eb9a66e1b84a63903e3133b5c6d96f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size  9\n",
      "embeddings generated : [True, True, True, True, True, True, True, True, True]\n",
      "saving to file :\n",
      "saved\n",
      "Processing file (6, 'NOTI821CFF2E131AB64E17BF3F50101EDE56F0.pdf'):\n",
      "extracted pages\n",
      "split documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ee1926b0ed4f5fa170e9850b5a63c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size  8\n",
      "embeddings generated : [True, True, True, True, True, True, True, True]\n",
      "saving to file :\n",
      "saved\n",
      "Processing file (7, 'NT13E6531B10CAE642489F29EE38E10C92E7.pdf'):\n",
      "extracted pages\n",
      "split documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce4b724305043519bed1a80bcebd9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size  13\n",
      "embeddings generated : [True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "saving to file :\n",
      "saved\n",
      "Processing file (8, 'NOTI103NSFR2912202346EE2A0CB6BD4705850B8F8AB3AA8D75.pdf'):\n",
      "extracted pages\n",
      "split documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3286466a7faf457083a922247669ead8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size  15\n",
      "embeddings generated : [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "saving to file :\n",
      "saved\n",
      "Processing file (9, 'NT19B668901332F243BEBDFD0DEB77E17F68.pdf'):\n",
      "extracted pages\n",
      "split documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe09e599ff3b47d8b0ac01f9233aa742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size  38\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 123\u001b[0m\n\u001b[1;32m    121\u001b[0m splits_1 \u001b[38;5;241m=\u001b[39m split_documents(splitter,documents_1)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 123\u001b[0m is_successful, question_chunk_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mencode_text_to_embedding_batched\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m\u001b[49m\u001b[43msentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplits_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43mapi_calls_per_second\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAPI_CALLS_PER_SECOND\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mITEMS_PER_REQUEST\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings generated :\u001b[39m\u001b[38;5;124m\"\u001b[39m, is_successful)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Save embeddings to jsonl format\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m, in \u001b[0;36mencode_text_to_embedding_batched\u001b[0;34m(sentences, api_calls_per_second, batch_size)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     58\u001b[0m     batches, total\u001b[38;5;241m=\u001b[39mmath\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(sentences) \u001b[38;5;241m/\u001b[39m batch_size), position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     59\u001b[0m ):\n\u001b[1;32m     60\u001b[0m     futures\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     61\u001b[0m         executor\u001b[38;5;241m.\u001b[39msubmit(functools\u001b[38;5;241m.\u001b[39mpartial(encode_texts_to_embeddings), batch)\n\u001b[1;32m     62\u001b[0m     )\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds_per_job\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[1;32m     66\u001b[0m     embeddings_list\u001b[38;5;241m.\u001b[39mextend(future\u001b[38;5;241m.\u001b[39mresult())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from pypdf import PdfReader\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "import functools\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Generator, List, Tuple\n",
    "from typing import Optional, TypeVar\n",
    "import math\n",
    "from typing import Any\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "#Vertex AI embedding\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko\")\n",
    "def encode_texts_to_vertex_embeddings(sentences: List[str]) -> List[Optional[List[float]]]:\n",
    "    print(\"batch size \", len(sentences))\n",
    "    try:\n",
    "        embeddings = model.get_embeddings(sentences)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception as e:\n",
    "        print(\"exception\", e)\n",
    "        return [None for _ in range(len(sentences))]\n",
    "    \n",
    "\n",
    "# Generator function to yield batches of sentences\n",
    "def generate_batches(\n",
    "    sentences: List[str], batch_size: int\n",
    ") -> Generator[List[str], None, None]:\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        yield sentences[i : i + batch_size]\n",
    "\n",
    "\n",
    "def encode_text_to_embedding_batched(\n",
    "    sentences: List[str], api_calls_per_second: float = 1.0, batch_size: int = 100\n",
    ") -> Tuple[List[bool], np.ndarray]:\n",
    "\n",
    "    embeddings_list: List[List[float]] = []\n",
    "    # Prepare the batches using a generator\n",
    "    batches = generate_batches(sentences, batch_size)\n",
    "\n",
    "    seconds_per_job = int(1 / api_calls_per_second)\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for batch in tqdm(\n",
    "            batches, total=math.ceil(len(sentences) / batch_size), position=0\n",
    "        ):\n",
    "            futures.append(\n",
    "                executor.submit(functools.partial(encode_texts_to_embeddings), batch)\n",
    "            )\n",
    "            time.sleep(seconds_per_job)\n",
    "\n",
    "        for future in futures:\n",
    "            embeddings_list.extend(future.result())\n",
    "\n",
    "    is_successful = [\n",
    "        embedding is not None for sentence, embedding in zip(sentences, embeddings_list)\n",
    "    ]\n",
    "    embeddings_list_successful = np.squeeze(\n",
    "        np.stack([embedding for embedding in embeddings_list if embedding is not None])\n",
    "    )\n",
    "    return is_successful, embeddings_list_successful\n",
    "\n",
    "# block to process entire directory of pdf docs\n",
    "\n",
    "embeddings_file_path = Path(\"/home/jupyter/rbi-bot/embeddings/\")\n",
    "\n",
    "# Create a rate limit of 5 requests per minute. default quota\n",
    "API_CALLS_PER_SECOND = 0.08\n",
    "# According to the docs, each request can process 5 instances per request\n",
    "ITEMS_PER_REQUEST = 100\n",
    "\n",
    "\n",
    "documents_directory = '/home/jupyter/rbi-bot/rbi-docs'\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "def split_documents(splitter, documents_1):\n",
    "    # Split the documents using the provided splitter\n",
    "    split_1 = splitter.split_text(documents_1)\n",
    "    # Create documents from the split chunks\n",
    "    #split_1 = splitter.create_documents(split_1)\n",
    "    return split_1\n",
    "\n",
    "vector_db = None\n",
    "for i, filename in enumerate(os.listdir(documents_directory)):\n",
    "    documents_1 = ''\n",
    "    # Create the full file path\n",
    "    filepath = os.path.join(documents_directory, filename)\n",
    "    \n",
    "    chunk_path = embeddings_file_path.joinpath(\n",
    "        f\"{embeddings_file_path.stem}_{filename}.json\"\n",
    "    )\n",
    "    \n",
    "    # Check if the file is a file and not a directory\n",
    "    if os.path.isfile(filepath):\n",
    "        # Open the file\n",
    "        with open(filepath, 'r') as file:\n",
    "            # Read the contents of the file\n",
    "            print(f\"Processing file {i,filename}:\")\n",
    "            reader = PdfReader(filepath)\n",
    "            for page in reader.pages:\n",
    "                documents_1 += page.extract_text()\n",
    "        # Implement embeddings\n",
    "        print(f\"extracted pages\")\n",
    "        splits_1 = split_documents(splitter,documents_1)\n",
    "        print(f\"split documents\")\n",
    "        is_successful, question_chunk_embeddings = encode_text_to_embedding_batched(\n",
    "        sentences=splits_1,\n",
    "        api_calls_per_second=API_CALLS_PER_SECOND,\n",
    "        batch_size=ITEMS_PER_REQUEST,\n",
    "        )\n",
    "        print(f\"embeddings generated :\", is_successful)\n",
    "        # Save embeddings to jsonl format\n",
    "        print(f\"saving to file :\")\n",
    "        with open(chunk_path, \"a\") as f:\n",
    "            # Append to file\n",
    "            embeddings_formatted = [\n",
    "                json.dumps(\n",
    "                    {\n",
    "                        \"document-id\": filename, \n",
    "                        \"chunk-seq\": str(num),\n",
    "                        \"chunk-id\" : filename + \"_\" + str(num),\n",
    "                        \"chunk-text\": splits_1[num],\n",
    "                        \"chunk-page-no\": \"0\",\n",
    "                        \"chunk-embedding\":  [str(value) for value in embedding]\n",
    "                    }\n",
    "                )\n",
    "                + \"\\n\"\n",
    "                for num,embedding in enumerate(question_chunk_embeddings)\n",
    "            ]\n",
    "            f.writelines(embeddings_formatted)\n",
    "        print(f\"saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e01d67c-e43f-421a-9ac0-d4a8995acb38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
