{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada8f2ff-4116-423f-aed6-9acd132f2dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from pypdf import PdfReader\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "import functools\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Generator, List, Tuple\n",
    "from typing import Optional, TypeVar\n",
    "import math\n",
    "from typing import Any\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "# Single File read block\n",
    "# Specify the directory you want to read\n",
    "\n",
    "documents_1 = ''\n",
    "documents_directory = '/home/jupyter/rbi-bot/rbi-docs'\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "def split_documents(splitter, documents_1):\n",
    "    # Split the documents using the provided splitter\n",
    "    split_1 = splitter.split_text(documents_1)\n",
    "    # Create documents from the split chunks\n",
    "    split_1 = splitter.create_documents(split_1)\n",
    "    return split_1\n",
    "\n",
    "vector_db = None\n",
    "filepath = '/home/jupyter/rbi-bot/rbi-docs/01MC01042024E0D6B768164C41678A616F743BF7426B 2.pdf'\n",
    "with open(filepath, 'r') as file:\n",
    "    # Read the contents of the file\n",
    "    reader = PdfReader(filepath)\n",
    "    for page in reader.pages:\n",
    "        documents_1 += page.extract_text()\n",
    "        print(f\"generated documents\")\n",
    "    splits_1 = split_documents(splitter,documents_1)\n",
    "    print(f\"split documents\")\n",
    "    #extension_db = FAISS.from_documents(splits_1, model)\n",
    "    #extension_db.save_local(vector_db_directory) \n",
    "    #print(splits_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c0f23-34d8-4b6a-ad14-837ab8f2912e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ffe56-f8fd-4806-94e5-54ec4242ea96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#OpenAI embedding\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"sk-rbi-bot-service-account-xJqcUMGqeOw7uLU86lOFT3BlbkFJdn7MkxPGycAAL8bzlkzx\")\n",
    "\n",
    "\n",
    "def encode_texts_to_openAI_embeddings(sentences: List[str]) -> List[Optional[List[float]]]:\n",
    "    print(\"batch size \", len(sentences))\n",
    "    try:\n",
    "        response = client.embeddings.create(input = sentences, model=\"text-embedding-3-small\").data\n",
    "        embeddings= [obj[embedding] for obj in response]\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception as e:\n",
    "        print(\"exception\", e)\n",
    "        return [None for _ in range(len(sentences))]\n",
    "    \n",
    "embeddings = encode_texts_to_openAI_embeddings([\"this is a text1\", \"this is a text 2\"])\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4fd2f-4065-49f4-9225-7206a07da176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b972f-52e1-4801-a4d3-906ba572a3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
