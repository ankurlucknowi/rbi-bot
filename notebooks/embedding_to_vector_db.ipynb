{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3032083-68bc-43f9-a12d-28c82fed7da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3.10 -m venv chromadb_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc82440-500f-4304-a283-82ee76867b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!source chromadb_env/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49811337-8f7f-4262-9055-dad0529f388b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b1c562-9edc-4282-8df1-e2168ece782e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8e49cd-000e-4f3a-a482-3e2376b28968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "client = chromadb.Client(settings=chromadb.config.Settings(persist_directory=\"/home/jupyter/rbi-bot/chromadb_data\"))\n",
    "\n",
    "# Load SentenceTransformer model\n",
    "model_name = \"all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Get or create your collection\n",
    "collection_name = \"rbi_embeddings\"\n",
    "collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "def read_embeddings(file_path):\n",
    "    \"\"\"Reads embeddings from a JSON file and converts them to lists of floats.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line)\n",
    "                # Convert embeddings to a list of floats\n",
    "                data['chunk-embedding'] = [float(x) for x in data['chunk-embedding']] \n",
    "                yield data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON format in file: {file_path}\")\n",
    "        \n",
    "def purge_data(collection):\n",
    "    try:\n",
    "        # Fetch all existing IDs in the collection\n",
    "        existing_ids = collection.get()[\"ids\"]\n",
    "\n",
    "        # If there are IDs to delete, delete them\n",
    "        if existing_ids:\n",
    "            collection.delete(ids=existing_ids) \n",
    "\n",
    "    except KeyError:\n",
    "        # If the collection is empty, the 'ids' key won't exist\n",
    "        pass\n",
    "\n",
    "def add_embeddings_to_chroma(embeddings_directory, collection, batch_size=1000):\n",
    "    \"\"\"Adds embeddings from JSON files to a ChromaDB collection.\"\"\"\n",
    "    \n",
    "    # Delete all existing data from the collection\n",
    "    purge_data(collection)\n",
    "    \n",
    "    seen_ids = set()\n",
    "    batch_data = []\n",
    "\n",
    "    total_embeddings_added = 0 \n",
    "    for file_path in embeddings_directory.glob(\"*.json\"):\n",
    "        for embedding_data in read_embeddings(file_path):\n",
    "            try:\n",
    "                unique_id = f\"{embedding_data['document-id']}_{embedding_data['chunk-id']}\"\n",
    "\n",
    "                if unique_id in seen_ids:\n",
    "                    # print(f\"Skipping duplicate ID: {unique_id}\")\n",
    "                    continue  # Skip this embedding\n",
    "\n",
    "                seen_ids.add(unique_id)\n",
    "\n",
    "                batch_data.append(\n",
    "                    {\n",
    "                        \"ids\": unique_id,\n",
    "                        \"embedding\": embedding_data[\"chunk-embedding\"],\n",
    "                        \"metadata\": {\n",
    "                            \"document_id\": embedding_data[\"document-id\"],\n",
    "                            \"text\": embedding_data[\"chunk-text\"],\n",
    "                        },\n",
    "                        \"document\": embedding_data[\"chunk-text\"],\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                if len(batch_data) >= batch_size:\n",
    "                    # Add all the data in batch_data to the collection\n",
    "                    ids = []\n",
    "                    embeddings = []\n",
    "                    metadatas = []\n",
    "                    documents = []\n",
    "                    for data in batch_data:\n",
    "                        ids.append(data[\"ids\"])\n",
    "                        embeddings.append(data[\"embedding\"])\n",
    "                        metadatas.append(data[\"metadata\"])\n",
    "                        documents.append(data[\"document\"])\n",
    "                    collection.add(ids=ids, embeddings=embeddings, metadatas=metadatas, documents=documents)\n",
    "                    total_embeddings_added += len(batch_data)\n",
    "                    batch_data = []  \n",
    "\n",
    "            except KeyError as e:\n",
    "                print(f\"Missing key in embedding data: {e}\")\n",
    "\n",
    "    # Add any remaining embeddings\n",
    "    if batch_data:\n",
    "         # Add all the data in batch_data to the collection\n",
    "        ids = []\n",
    "        embeddings = []\n",
    "        metadatas = []\n",
    "        documents = []\n",
    "        for data in batch_data:\n",
    "            ids.append(data[\"ids\"])\n",
    "            embeddings.append(data[\"embedding\"])\n",
    "            metadatas.append(data[\"metadata\"])\n",
    "            documents.append(data[\"document\"])\n",
    "        collection.add(ids=ids, embeddings=embeddings, metadatas=metadatas, documents=documents)\n",
    "        total_embeddings_added += len(batch_data)\n",
    "        \n",
    "    print(\"Embeddings added to Chroma DB successfully!\")\n",
    "    print(f\"Total Embeddings Added: {total_embeddings_added}\")  # Print total count\n",
    "\n",
    "def search_rbi_documents(query_text, n_results=3, collection=collection):\n",
    "    \"\"\"Searches RBI documents based on a query.\"\"\"\n",
    "    query_embedding = model.encode(query_text).tolist()\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"distances\", \"metadatas\"]\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb15bd49-fca2-4a75-a9ba-d78d4385952d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings added to Chroma DB successfully!\n",
      "Total Embeddings Added: 1503\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "embeddings_directory = Path(\"/home/jupyter/rbi-bot/embeddings/\")\n",
    "add_embeddings_to_chroma(embeddings_directory, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c6160-67b0-431d-b491-fcb5a0bf6d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"What are the guidelines for opening a new bank account?\"\n",
    "search_results = search_rbi_documents(query_text)\n",
    "\n",
    "for i in range(len(search_results['ids'][0])):  # Iterate over the indices of the results\n",
    "    document_id = search_results['ids'][0][i]      # Get the document ID at the index\n",
    "    document = search_results['documents'][0][i]   # Get the document text\n",
    "    distance = search_results['distances'][0][i]   # Get the distance\n",
    "    metadata = search_results['metadatas'][0][i]   # Get the metadata (dictionary)\n",
    "    \n",
    "    print(f\"\\nDocument: {document}\")\n",
    "    print(f\"Distance: {distance}\")\n",
    "    print(f\"Metadata: {metadata}\")  # No need for [0] here, as metadata is already a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec0d62-ed10-4a10-bd16-d866e3bb1b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection_info = collection.get()\n",
    "print(collection_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe361aef-f1aa-453a-9f60-a53610429afb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment='' chroma_api_impl='chromadb.api.segment.SegmentAPI' chroma_server_nofile=None chroma_server_thread_pool_size=40 tenant_id='default' topic_namespace='default' chroma_server_host=None chroma_server_headers=None chroma_server_http_port=None chroma_server_ssl_enabled=False chroma_server_ssl_verify=None chroma_server_api_default_path='/api/v1' chroma_server_cors_allow_origins=[] is_persistent=False persist_directory='/home/jupyter/rbi-bot/chromadb_data' chroma_memory_limit_bytes=0 chroma_segment_cache_policy=None allow_reset=False chroma_auth_token_transport_header=None chroma_client_auth_provider=None chroma_client_auth_credentials=None chroma_server_auth_ignore_paths={'/api/v1': ['GET'], '/api/v1/heartbeat': ['GET'], '/api/v1/version': ['GET']} chroma_overwrite_singleton_tenant_database_access_from_auth=False chroma_server_authn_provider=None chroma_server_authn_credentials=None chroma_server_authn_credentials_file=None chroma_server_authz_provider=None chroma_server_authz_config=None chroma_server_authz_config_file=None chroma_product_telemetry_impl='chromadb.telemetry.product.posthog.Posthog' chroma_telemetry_impl='chromadb.telemetry.product.posthog.Posthog' anonymized_telemetry=True chroma_otel_collection_endpoint='' chroma_otel_service_name='chromadb' chroma_otel_collection_headers={} chroma_otel_granularity=None migrations='apply' migrations_hash_algorithm='md5' chroma_segment_directory_impl='chromadb.segment.impl.distributed.segment_directory.RendezvousHashSegmentDirectory' chroma_memberlist_provider_impl='chromadb.segment.impl.distributed.segment_directory.CustomResourceMemberlistProvider' worker_memberlist_name='query-service-memberlist' chroma_server_grpc_port=None chroma_sysdb_impl='chromadb.db.impl.sqlite.SqliteDB' chroma_producer_impl='chromadb.db.impl.sqlite.SqliteDB' chroma_consumer_impl='chromadb.db.impl.sqlite.SqliteDB' chroma_segment_manager_impl='chromadb.segment.impl.manager.local.LocalSegmentManager' chroma_quota_provider_impl=None chroma_rate_limiting_provider_impl=None chroma_db_impl=None chroma_collection_assignment_policy_impl='chromadb.ingest.impl.simple_policy.SimpleAssignmentPolicy' chroma_coordinator_host='localhost' chroma_logservice_host='localhost' chroma_logservice_port=50052\n"
     ]
    }
   ],
   "source": [
    "print(client.get_settings())"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
