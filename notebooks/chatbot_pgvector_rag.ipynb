{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70d994-4e1c-4b42-9e13-7088928f7a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#!pip install openai sentence-transformers==2.2.2 pypdf SQLAlchemy\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#!pip install psycopg2\n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from openai import OpenAI\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "def get_embedding(text, embedding_model, out_type=\"array\"):\n",
    "    text = text.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "    embedding = embedding_model.encode(text)\n",
    "    if out_type == \"list\":\n",
    "        return embedding\n",
    "    elif out_type == \"array\":\n",
    "        return np.array(embedding)\n",
    "        \n",
    "def similarity_search_from_emb(emb, engine, match_threshold=0.75, match_count=10):\n",
    "    formatted_str = ', '.join(map(str, emb))\n",
    "    formatted_str = f\"[{formatted_str}]\"\n",
    "    sql = f\"\"\"WITH cte AS (SELECT document_domain,document_name, page_number, sequence, text, (embedding_1024 <#> '{formatted_str}') as similarity \n",
    "    FROM document_embeddings\n",
    "    ORDER BY similarity asc\n",
    "    LIMIT {match_count})\n",
    "    SELECT * FROM cte\n",
    "    WHERE similarity < -{match_threshold}\"\"\"\n",
    "    df = None\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "        df = pd.read_sql(sql=sql, con=engine)    \n",
    "    df.similarity *= -1.0\n",
    "    return df\n",
    "\n",
    "def similarity_search(text, engine,embedding_model,match_threshold=0.75, match_count=10):\n",
    "    start = perf_counter()\n",
    "    #emb = get_embedding(text, embedding_model,out_type=\"list\")\n",
    "    emb = get_embedding(text, embedding_model,out_type=\"array\")\n",
    "    end = perf_counter()\n",
    "    elapsed_time = end - start\n",
    "    start = perf_counter()\n",
    "    df = similarity_search_from_emb(emb, engine, match_threshold=0.75, match_count=match_count)\n",
    "    end = perf_counter()\n",
    "    elapsed_time = end - start\n",
    "    return df\n",
    "\n",
    "def get_surrounding_chunks(engine,document_domain,document_name,sequence, N):\n",
    "    # SQL query to fetch surrounding chunks\n",
    "    seq_min = sequence - N\n",
    "    seq_max = sequence + N\n",
    "    query = f\"\"\"\n",
    "        SELECT document_domain,document_name,Page_Number,sequence,text  FROM document_embeddings\n",
    "        WHERE document_domain = '{document_domain}' AND\n",
    "              document_name = '{document_name}' AND\n",
    "              sequence BETWEEN '{seq_min}' AND '{seq_max}'\n",
    "        ORDER BY document_domain,document_name,sequence ASC\n",
    "    \"\"\" \n",
    "    result = None\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "        result = pd.read_sql(sql=query, con=engine)    \n",
    "    return result\n",
    "    \n",
    "\n",
    "def get_prompt_context_using_rag(query,engine,embedding_model):\n",
    "    df = similarity_search(query,engine,embedding_model)\n",
    "    top_rows = df.head(3)\n",
    "    N = 3\n",
    "    all_chunks = pd.DataFrame()\n",
    "    pd.set_option('display.max_colwidth', None)  # For pandas versions 1.0 and later\n",
    "    for index, row in top_rows.iterrows():\n",
    "        surrounding_chunks = get_surrounding_chunks(engine,row['document_domain'], row['document_name'], row['sequence'], N)\n",
    "        all_chunks = pd.concat([all_chunks, surrounding_chunks], ignore_index=True)\n",
    "    all_chunks.drop_duplicates(subset=['document_name', 'sequence'])\n",
    "    all_chunks.sort_values(by=['document_name', 'page_number', 'sequence'], inplace=True)\n",
    "    all_chunks['text'] = all_chunks.sort_values('sequence').groupby(['document_name', 'page_number'])['text'].transform(lambda x: ' '.join(x))\n",
    "    all_chunks = all_chunks.drop_duplicates(subset=['document_name', 'page_number'])\n",
    "    all_chunks.reset_index(drop=True, inplace=True)\n",
    "    prompt_input = all_chunks.apply(lambda row: f\"Document: {row['document_name']}, Page: {row['page_number']}, Content: {row['text']}\", axis=1).tolist()\n",
    "    return prompt_input[0]\n",
    "\n",
    "def get_response(prompt_input,llm_client,embedding_model):\n",
    "    global messages\n",
    "    messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\\\n",
    "                 And will always answer the question asked in 'Question:' and \\\n",
    "                 will quote the Document and Page number at the end of the answer,\\\n",
    "                 the Document: and Page: fields precede the content based on which you will answer.\"},\n",
    "                {\"role\": \"user\", \"content\": ''.join(prompt_input)}\n",
    "          ]\n",
    "\n",
    "    response = llm_client.chat.completions.create(\n",
    "                            model = \"gpt-3.5-turbo\",\n",
    "                            messages = messages,\n",
    "                            temperature=0.2,               \n",
    "                     )\n",
    "    \n",
    "    #response_msg = \"llm response here\"\n",
    "    response_msg = response.choices[0].message.content\n",
    "    messages = messages + [{\"role\":'assistant', 'content': response_msg}]\n",
    "    return response_msg\n",
    "\n",
    "\n",
    "def get_answer(query,openai_clien,engine,embedding_model):\n",
    "    query_string = get_prompt_context_using_rag(query,engine,embedding_model)\n",
    "    query_string = query_string + f\" ques: {query}\"\n",
    "    answer = get_response(query_string,openai_client,embedding_model)\n",
    "    return answer\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "#!pip install dotenv\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "def setup():\n",
    "    try:\n",
    "        print(\"inside init\")\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        document_domain = \"RBI_Guidelines\"\n",
    "        dbname = 'rbi_bot'\n",
    "        dbuser = 'rbi_bot_user'\n",
    "        dbpassword = 'password'\n",
    "        dbhost = '127.0.0.1'\n",
    "        dbport = 5432\n",
    "        chunk_size = 400\n",
    "\n",
    "        embedding_model_name = 'thenlper/gte-large'\n",
    "        embedding_model = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "        db_conn_str = f\"postgresql://{dbuser}:{dbpassword}@{dbhost}:{dbport}/{dbname}\"\n",
    "        global sql_engine\n",
    "        sql_engine = create_engine(db_conn_str)\n",
    "        global openai_client\n",
    "        openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        print(f\"{sql_engine}-{openai_client}-{embedding_model}\")\n",
    "        return openai_client,sql_engine,embedding_model\n",
    "    except Exception as e:\n",
    "        print(\"Error during initialization:\", e)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "#print(get_answer(\"Is there a limit to the interest-rate I can charge a customer on a loan? \",openai_client,sql_engine))\n",
    "#\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#print(get_answer(\"Can a co-lending partner issue a gold-loan at the customer doorstep?\",openai_client,sql_engine))\n",
    "#print(get_answer(\"Can I host a payment gateway for Indian customers using a server located in Pakistan?\",openai_client,sql_engine))\n",
    "#print(get_answer(\"Can I create a special scheme for SC ST customers?\",openai_client,sql_engine))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
